<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parallelization · Trixi.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://trixi-framework.github.io/Trixi.jl/stable/parallelization/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Trixi.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Getting started</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../visualization/">Visualization</a></li><li><a class="tocitem" href="../restart/">Restart simulation</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/introduction/">Introduction</a></li><li><a class="tocitem" href="../tutorials/scalar_linear_advection_1d/">1 Introduction to DG methods</a></li><li><a class="tocitem" href="../tutorials/DGSEM_FluxDiff/">2 DGSEM with flux differencing</a></li><li><a class="tocitem" href="../tutorials/shock_capturing/">3 Shock capturing with flux differencing and stage limiter</a></li><li><a class="tocitem" href="../tutorials/non_periodic_boundaries/">4 Non-periodic boundaries</a></li><li><a class="tocitem" href="../tutorials/DGMulti_1/">5 DG schemes via <code>DGMulti</code> solver</a></li><li><a class="tocitem" href="../tutorials/DGMulti_2/">6 Other SBP schemes (FD, CGSEM) via <code>DGMulti</code> solver</a></li><li><a class="tocitem" href="../tutorials/upwind_fdsbp/">7 Upwind FD SBP schemes</a></li><li><a class="tocitem" href="../tutorials/adding_new_scalar_equations/">8 Adding a new scalar conservation law</a></li><li><a class="tocitem" href="../tutorials/adding_nonconservative_equation/">9 Adding a non-conservative equation</a></li><li><a class="tocitem" href="../tutorials/parabolic_terms/">10 Parabolic terms</a></li><li><a class="tocitem" href="../tutorials/adding_new_parabolic_terms/">11 Adding new parabolic terms</a></li><li><a class="tocitem" href="../tutorials/adaptive_mesh_refinement/">12 Adaptive mesh refinement</a></li><li><a class="tocitem" href="../tutorials/structured_mesh_mapping/">13 Structured mesh with curvilinear mapping</a></li><li><a class="tocitem" href="../tutorials/hohqmesh_tutorial/">14 Unstructured meshes with HOHQMesh.jl</a></li><li><a class="tocitem" href="../tutorials/time_stepping/">15 Explicit time stepping</a></li><li><a class="tocitem" href="../tutorials/differentiable_programming/">16 Differentiable programming</a></li></ul></li><li><span class="tocitem">Basic building blocks</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Meshes</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../meshes/tree_mesh/">Tree mesh</a></li><li><a class="tocitem" href="../meshes/structured_mesh/">Structured mesh</a></li><li><a class="tocitem" href="../meshes/unstructured_quad_mesh/">Unstructured mesh</a></li><li><a class="tocitem" href="../meshes/p4est_mesh/">P4est-based mesh</a></li><li><a class="tocitem" href="../meshes/dgmulti_mesh/">DGMulti mesh</a></li></ul></li><li><a class="tocitem" href="../time_integration/">Time integration</a></li><li><a class="tocitem" href="../callbacks/">Callbacks</a></li></ul></li><li><span class="tocitem">Advanced topics &amp; developers</span><ul><li><a class="tocitem" href="../conventions/">Conventions</a></li><li><a class="tocitem" href="../development/">Development</a></li><li><a class="tocitem" href="../github-git/">GitHub &amp; Git</a></li><li><a class="tocitem" href="../styleguide/">Style guide</a></li><li><a class="tocitem" href="../testing/">Testing</a></li><li><a class="tocitem" href="../performance/">Performance</a></li><li class="is-active"><a class="tocitem" href>Parallelization</a><ul class="internal"><li><a class="tocitem" href="#Shared-memory-parallelization-with-threads"><span>Shared-memory parallelization with threads</span></a></li><li><a class="tocitem" href="#Distributed-computing-with-MPI"><span>Distributed computing with MPI</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../troubleshooting/">Troubleshooting and FAQ</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference-trixi/">Trixi.jl</a></li><li><a class="tocitem" href="../reference-trixi2vtk/">Trixi2Vtk.jl</a></li></ul></li><li><a class="tocitem" href="../authors/">Authors</a></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../code_of_conduct/">Code of Conduct</a></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Advanced topics &amp; developers</a></li><li class="is-active"><a href>Parallelization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parallelization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/trixi-framework/Trixi.jl/blob/main/docs/src/parallelization.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Parallelization"><a class="docs-heading-anchor" href="#Parallelization">Parallelization</a><a id="Parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelization" title="Permalink"></a></h1><h2 id="Shared-memory-parallelization-with-threads"><a class="docs-heading-anchor" href="#Shared-memory-parallelization-with-threads">Shared-memory parallelization with threads</a><a id="Shared-memory-parallelization-with-threads-1"></a><a class="docs-heading-anchor-permalink" href="#Shared-memory-parallelization-with-threads" title="Permalink"></a></h2><p>Many compute-intensive loops in Trixi.jl are parallelized using the <a href="https://docs.julialang.org/en/v1/manual/multi-threading/">multi-threading</a> support provided by Julia. You can recognize those loops by the <code>@threaded</code> macro prefixed to them, e.g.,</p><pre><code class="language-julia hljs">@threaded for element in eachelement(dg, cache)
  ...
end</code></pre><p>This will statically assign an equal iteration count to each available thread.</p><p>To use multi-threading, you need to tell Julia at startup how many threads you want to use by either setting the environment variable <code>JULIA_NUM_THREADS</code> or by providing the <code>-t/--threads</code> command line argument. For example, to start Julia with four threads, start Julia with</p><pre><code class="language-bash hljs">julia --threads=4</code></pre><p>If both the environment variable and the command line argument are specified at the same time, the latter takes precedence.</p><p>If you use time integration methods from  <a href="https://github.com/SciML/OrdinaryDiffEq.jl">OrdinaryDiffEq.jl</a> and want to use multiple threads therein, you need to set the keyword argument <code>thread=OrdinaryDiffEq.True()</code> of the algorithms, as described in the <a href="../time_integration/#time-integration">section on time integration methods</a>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Not everything is parallelized yet and there are likely opportunities to improve scalability. Multi-threading isn&#39;t considered part of the public API of Trixi.jl yet.</p></div></div><h2 id="Distributed-computing-with-MPI"><a class="docs-heading-anchor" href="#Distributed-computing-with-MPI">Distributed computing with MPI</a><a id="Distributed-computing-with-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-computing-with-MPI" title="Permalink"></a></h2><p>In addition to the shared memory parallelization with multi-threading, Trixi.jl supports distributed parallelism via <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>, which leverages the Message Passing Interface (MPI). MPI.jl comes with its own MPI library binaries such that there is no need to install MPI yourself. However, it is also possible to instead use an existing MPI installation, which is recommended if you are running MPI programs on a cluster or supercomputer (<a href="https://juliaparallel.github.io/MPI.jl/stable/configuration/">see the MPI.jl docs</a> to find out how to select the employed MPI library). Additional notes on how to use a system-provided MPI installation with Trixi.jl can be found in the following subsection.</p><div class="admonition is-warning"><header class="admonition-header">Work in progress</header><div class="admonition-body"><p>MPI-based parallelization is work in progress and not finished yet. Nothing related to MPI is part of the official API of Trixi.jl yet.</p></div></div><h3 id="parallel_system_MPI"><a class="docs-heading-anchor" href="#parallel_system_MPI">Using a system-provided MPI installation</a><a id="parallel_system_MPI-1"></a><a class="docs-heading-anchor-permalink" href="#parallel_system_MPI" title="Permalink"></a></h3><p>When using Trixi.jl with a system-provided MPI backend the underlying <a href="https://github.com/cburstedde/p4est"><code>p4est</code></a> and <a href="https://github.com/DLR-AMR/t8code"><code>t8code</code></a> libraries need to be compiled with the same MPI installation. Therefore, you also need to use system-provided <code>p4est</code> and <code>t8code</code> installations (for notes on how to install <code>p4est</code> and <code>t8code</code> see e.g. <a href="https://github.com/cburstedde/p4est/blob/master/README">here</a> and <a href="https://github.com/DLR-AMR/t8code/wiki/Installation">here</a>, use the configure option <code>--enable-mpi</code>). Note that <code>t8code</code> already comes with a <code>p4est</code> installation, so it suffices to install <code>t8code</code>. In addition, <a href="https://github.com/trixi-framework/P4est.jl">P4est.jl</a> and <a href="https://github.com/DLR-AMR/T8code.jl">T8code.jl</a> need to be configured to use the custom installations. Follow the steps described <a href="https://github.com/DLR-AMR/T8code.jl/blob/main/README.md#installation">here</a> and <a href="https://github.com/trixi-framework/P4est.jl/blob/main/README.md#installation">here</a> for the configuration. The paths that point to <code>libp4est.so</code> (and potentially to <code>libsc.so</code>) need to be the same for P4est.jl and T8code.jl. This could e.g. be <code>libp4est.so</code> that usually can be found in <code>lib/</code> or <code>local/lib/</code> in the installation directory of <code>t8code</code>. In total, in your active Julia project you should have a LocalPreferences.toml file with sections <code>[MPIPreferences]</code>, <code>[T8code]</code> and <code>[P4est]</code> as well as an entry <code>MPIPreferences</code> in your Project.toml to use a custom MPI installation.</p><h3 id="parallel_usage"><a class="docs-heading-anchor" href="#parallel_usage">Usage</a><a id="parallel_usage-1"></a><a class="docs-heading-anchor-permalink" href="#parallel_usage" title="Permalink"></a></h3><p>To start Trixi.jl in parallel with MPI, there are three options:</p><ol><li><p><strong>Run from the REPL with <code>mpiexec()</code>:</strong> You can start a parallel execution directly from the REPL by executing</p><pre><code class="language-julia hljs">julia&gt; using MPI

julia&gt; mpiexec() do cmd
         run(`$cmd -n 3 $(Base.julia_cmd()) --threads=1 --project=@. -e &#39;using Trixi; trixi_include(default_example())&#39;`)
       end</code></pre><p>The parameter <code>-n 3</code> specifies that Trixi.jl should run with three processes (or <em>ranks</em> in MPI parlance) and should be adapted to your available computing resources and problem size. The <code>$(Base.julia_cmd())</code> argument ensures that Julia is executed in parallel with the same optimization level etc. as you used for the REPL; if this is unnecessary or undesired, you can also just use <code>julia</code>.  Further, if you are not running Trixi.jl from a local clone but have installed it as a package, you need to omit the <code>--project=@.</code>.</p></li><li><p><strong>Run from the command line with <code>mpiexecjl</code>:</strong> Alternatively, you can use the <code>mpiexecjl</code> script provided by MPI.jl, which allows you to start Trixi.jl in parallel directly from the command line. As a preparation, you need to install the script <em>once</em> by running</p><pre><code class="language-julia hljs">julia&gt; using MPI

julia&gt; MPI.install_mpiexecjl(destdir=&quot;/somewhere/in/your/PATH&quot;)</code></pre><p>Then, to execute Trixi.jl in parallel, execute the following command from your command line:</p><pre><code class="language-bash hljs">mpiexecjl -n 3 julia --threads=1 --project=@. -e &#39;using Trixi; trixi_include(default_example())&#39;</code></pre></li><li><p><strong>Run interactively with <code>tmpi</code> (Linux/MacOS only):</strong> If you are on a Linux/macOS system, you have a third option which lets you run Julia in parallel interactively from the REPL. This comes in handy especially during development, as in contrast to the first two options, it allows to reuse the compilation cache and thus facilitates much faster startup times after the first execution. It requires <a href="https://github.com/tmux/tmux">tmux</a> and the <a href="https://www.open-mpi.org">OpenMPI</a> library to be installed before, both of which are usually available through a package manager. Once you have installed both tools, you need to configure MPI.jl to use the OpenMPI for your system, which is explained <a href="https://juliaparallel.org/MPI.jl/stable/configuration/#Using-a-system-provided-MPI-backend">here</a>. Then, you can download and install the <a href="https://github.com/Azrael3000/tmpi">tmpi</a> script by executing</p><pre><code class="language-bash hljs">curl https://raw.githubusercontent.com/Azrael3000/tmpi/master/tmpi -o /somewhere/in/your/PATH/tmpi</code></pre><p>Finally, you can start and control multiple Julia REPLs simultaneously by running</p><pre><code class="language-bash hljs">tmpi 3 julia --threads=1 --project=@.</code></pre><p>This will start Julia inside <code>tmux</code> three times and multiplexes all commands you enter in one REPL to all other REPLs (try for yourself to understand what it means). If you have no prior experience with <code>tmux</code>, handling the REPL this way feels slightly weird in the beginning. However, there is a lot of documentation for <code>tmux</code> <a href="https://github.com/tmux/tmux/wiki/Getting-Started">available</a> and once you get the hang of it, developing Trixi.jl in parallel becomes much smoother this way. Some helpful commands are the following. To close a single pane you can press <code>Ctrl+b</code> and then <code>x</code> followed by <code>y</code> to confirm. To quit the whole session you press <code>Ctrl+b</code> followed by <code>:kill-session</code>. Often you would like to scroll up. You can do that by pressing <code>Ctrl+b</code> and then <code>[</code>, which allows you to use the arrow keys to scroll up and down. To leave the scroll mode you press <code>q</code>. Switching between panes can be done by <code>Ctrl+b</code> followed by <code>o</code>. As of March 2022, newer versions of tmpi also support mpich, which is the default backend of MPI.jl (via MPICH_Jll.jl). To use this setup, you need to install <code>mpiexecjl</code> as described in the  <a href="https://juliaparallel.org/MPI.jl/v0.20/usage/#Julia-wrapper-for-mpiexec">documentation of MPI.jl</a> and make it available as <code>mpirun</code>, e.g., via a symlink of the form</p><pre><code class="language-bash hljs">ln -s ~/.julia/bin/mpiexecjl /somewhere/in/your/path/mpirun</code></pre><p>(assuming default installations).</p></li></ol><div class="admonition is-info"><header class="admonition-header">Hybrid parallelism</header><div class="admonition-body"><p>It is possible to combine MPI with shared memory parallelism via threads by starting Julia with more than one thread, e.g. by passing the command line argument <code>julia --threads=2</code> instead of <code>julia --threads=1</code> used in the examples above. In that case, you should make sure that your system supports the number of processes/threads that you try to start.</p></div></div><h3 id="parallel_performance"><a class="docs-heading-anchor" href="#parallel_performance">Performance</a><a id="parallel_performance-1"></a><a class="docs-heading-anchor-permalink" href="#parallel_performance" title="Permalink"></a></h3><p>For information on how to evaluate the parallel performance of Trixi.jl, please have a look at the <a href="../performance/#Performance-metrics-of-the-AnalysisCallback">Performance metrics of the <code>AnalysisCallback</code></a> section, specifically at the descriptions of the performance index (PID).</p><h3 id="Using-error-based-step-size-control-with-MPI"><a class="docs-heading-anchor" href="#Using-error-based-step-size-control-with-MPI">Using error-based step size control with MPI</a><a id="Using-error-based-step-size-control-with-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Using-error-based-step-size-control-with-MPI" title="Permalink"></a></h3><p>If you use error-based step size control (see also the section on <a href="../tutorials/time_stepping/#adaptive_step_sizes">error-based adaptive step sizes</a>) together with MPI you need to pass <code>internalnorm=ode_norm</code> and you should pass <code>unstable_check=ode_unstable_check</code> to OrdinaryDiffEq&#39;s <a href="https://docs.sciml.ai/DiffEqDocs/latest/basics/common_solver_opts/"><code>solve</code></a>, which are both included in <a href="../reference-trixi/#Trixi.ode_default_options-Tuple{}"><code>ode_default_options</code></a>.</p><h3 id="Using-parallel-input-and-output"><a class="docs-heading-anchor" href="#Using-parallel-input-and-output">Using parallel input and output</a><a id="Using-parallel-input-and-output-1"></a><a class="docs-heading-anchor-permalink" href="#Using-parallel-input-and-output" title="Permalink"></a></h3><p>Trixi.jl allows parallel I/O using MPI by leveraging parallel HDF5.jl. On most systems, this is enabled by default. Additionally, you can also use a local installation of the HDF5 library (with MPI support). For this, you first need to use a system-provided MPI library, see also <a href="#parallel_system_MPI">here</a> and you need to tell <a href="https://github.com/JuliaIO/HDF5.jl">HDF5.jl</a> to use this library. To do so with HDF5.jl v0.17 and newer, set the preferences <code>libhdf5</code> and <code>libhdf5_hl</code> to the local paths of the libraries <code>libhdf5</code> and <code>libhdf5_hl</code>, which can be done by</p><pre><code class="language-julia hljs">julia&gt; using Preferences, UUIDs
julia&gt; set_preferences!(
           UUID(&quot;f67ccb44-e63f-5c2f-98bd-6dc0ccc4ba2f&quot;), # UUID of HDF5.jl
           &quot;libhdf5&quot; =&gt; &quot;/path/to/your/libhdf5.so&quot;,
           &quot;libhdf5_hl&quot; =&gt; &quot;/path/to/your/libhdf5_hl.so&quot;, force = true)</code></pre><p>For more information see also the <a href="https://juliaio.github.io/HDF5.jl/stable/mpi/">documentation of HDF5.jl</a>. In total, you should have a file called LocalPreferences.toml in the project directory that contains a section <code>[MPIPreferences]</code>, a section <code>[HDF5]</code> with entries <code>libhdf5</code> and <code>libhdf5_hl</code>, a section <code>[P4est]</code> with the entry <code>libp4est</code> as well as a section <code>[T8code]</code> with the entries <code>libt8</code>, <code>libp4est</code> and <code>libsc</code>. If you use HDF5.jl v0.16 or older, instead of setting the preferences for HDF5.jl, you need to set the environment variable <code>JULIA_HDF5_PATH</code> to the path, where the HDF5 binaries are located and then call <code>]build HDF5</code> from Julia.</p><p>If HDF5 is not MPI-enabled, Trixi.jl will fall back on a less efficient I/O mechanism. In that case, all disk I/O is performed only on rank zero and data is distributed to/gathered from the other ranks using regular MPI communication.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../performance/">« Performance</a><a class="docs-footer-nextpage" href="../troubleshooting/">Troubleshooting and FAQ »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 12 September 2023 11:37">Tuesday 12 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
